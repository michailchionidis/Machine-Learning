{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ced0117",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Gradient Descent is an algorithm that can be used to estimate the values of the parametrs w and b to minize the cost function ($minJ(w,b)$). It is also used for training of Neural Network Models in Machine Learning (also called Deep Learning Models).\n",
    "<br> <br> ðŸ’¡ Gradient Descent can be used to minimize any function, not just the cost function in Linear Regression (for example, minimize functions that have more than 2 parameters, i.e. $minJ(w1,w2,...,wn,b)$).\n",
    "\n",
    "## ðŸŽ¯ Goals:\n",
    "- Learn about the Gradient Descent (how it works + equations)\n",
    "- Implement the Gradient Descent algorithm for linear regression with one variable in Python\n",
    "\n",
    "\n",
    "##  ðŸ“– Theory \n",
    "<br> <b> How does Gradient Descent work? </b> <br>\n",
    "<br> 1. Starts with some initial values of our parameters (w,b), for instance, set: $w=0$ and $b=0$\n",
    "<br> 2. Then keep changing the values of $w$ and $b$ a little bit, in order to reduce the cost: $J(w,b)$, until $J$ settles at or near the minimum value. \n",
    "<br> <br> <b> Notes: </b>\n",
    "<br> 1. For some functions, there might be more than one possible minimums!\n",
    "<br> 2. Based on the starting point you may end up in a different local minimum.\n",
    "<br> 3. What you actually do at every iterration, is a step (change to the values of the parameters) to that direction that leads to the maximum decrease of the (cost) function.\n",
    "<br> 4. The process ends when there is no step that you can take to any direction to decrease the value of the function.\n",
    "<img src = \"Gradient_Descent_1.png\" width=\"500\" height=\"340\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62903fae",
   "metadata": {},
   "source": [
    "## Gradient Descent Equation:\n",
    "Suppose that we want to minimize the Cost function: $$J(w,b) = \\frac{1}{1m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{1}$$ \n",
    "\n",
    "For this reason, at every step (iteration), the values of the parameters: w, b are modified by:\n",
    "\n",
    "$$\\begin{align*}\\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{2}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline\n",
    "\\end{align*}$$\n",
    "\n",
    "where:\n",
    "<br><br> the parameter <b>$a$</b> is called: <b>Learning Rate</b>\n",
    "<br><br> and the term: <b>$\\frac{\\partial J(w,b)}{\\partial w} \\$</b> is called <b>Derivative</b>.\n",
    "\n",
    "<img src = \"Gradient_Descent_maths.png\" width=\"500\" height=\"340\">\n",
    "\n",
    "In the above image, *Simultanious Update* means that the partial derivatives for all the parameters are calculated before updating any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca9f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform multiple runs of the gradient descent with different learning rates \n",
    "#and choose the valus of b and w that gives the total minimum of the function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
